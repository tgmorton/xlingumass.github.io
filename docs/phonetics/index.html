<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Phonetics Guides | XLing Labs | UMass</title><meta name=keywords content="P-Lab"><meta name=description content="Phoneme Categorization In a phoneme categorization experiment, a listener is presented with a single stimulus on each trial, and they have to assign that stimulus to one or another category. It&rsquo;s possible that the listener may be given more than just two categories to choose from but that&rsquo;s rare in the experiments run in our lab.
Usually, each stimulus is one of a sequence of stimuli that steps incrementally from a good instance of one category to a good instance of another."><meta name=author content="Christina Muxica"><link rel=canonical href=https://xlingumass.github.io/docs/phonetics/><link crossorigin=anonymous href=/assets/css/stylesheet.min.a14de8fd347f2728f906ae18144f69aa1415f5160efef0209c1cca416aebedf0.css integrity="sha256-oU3o/TR/Jyj5Bq4YFE9pqhQV9RYO/vAgnBzKQWrr7fA=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.2840b7fccd34145847db71a290569594bdbdb00047097f75d6495d162f5d7dff.js integrity="sha256-KEC3/M00FFhH23GikFaVlL29sABHCX911kldFi9dff8=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://www.umass.edu/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://www.umass.edu/favicon.ico><link rel=icon type=image/png sizes=32x32 href=https://www.umass.edu/favicon.ico><link rel=apple-touch-icon href=https://xlingumass.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://xlingumass.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,o,i,a,t,n,s){e.GoogleAnalyticsObject=t,e[t]=e[t]||function(){(e[t].q=e[t].q||[]).push(arguments)},e[t].l=1*new Date,n=o.createElement(i),s=o.getElementsByTagName(i)[0],n.async=1,n.src=a,s.parentNode.insertBefore(n,s)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Phonetics Guides"><meta property="og:description" content="Phoneme Categorization In a phoneme categorization experiment, a listener is presented with a single stimulus on each trial, and they have to assign that stimulus to one or another category. It&rsquo;s possible that the listener may be given more than just two categories to choose from but that&rsquo;s rare in the experiments run in our lab.
Usually, each stimulus is one of a sequence of stimuli that steps incrementally from a good instance of one category to a good instance of another."><meta property="og:type" content="article"><meta property="og:url" content="https://xlingumass.github.io/docs/phonetics/"><meta property="og:image" content="https://xlingumass.github.io/%3Cimage%20path/url%3E"><meta property="article:section" content="docs"><meta property="og:site_name" content="XLing Labs | UMass"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://xlingumass.github.io/%3Cimage%20path/url%3E"><meta name=twitter:title content="Phonetics Guides"><meta name=twitter:description content="Phoneme Categorization In a phoneme categorization experiment, a listener is presented with a single stimulus on each trial, and they have to assign that stimulus to one or another category. It&rsquo;s possible that the listener may be given more than just two categories to choose from but that&rsquo;s rare in the experiments run in our lab.
Usually, each stimulus is one of a sequence of stimuli that steps incrementally from a good instance of one category to a good instance of another."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Docs","item":"https://xlingumass.github.io/docs/"},{"@type":"ListItem","position":3,"name":"Phonetics Guides","item":"https://xlingumass.github.io/docs/phonetics/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Phonetics Guides","name":"Phonetics Guides","description":"Phoneme Categorization In a phoneme categorization experiment, a listener is presented with a single stimulus on each trial, and they have to assign that stimulus to one or another category. It\u0026rsquo;s possible that the listener may be given more than just two categories to choose from but that\u0026rsquo;s rare in the experiments run in our lab.\nUsually, each stimulus is one of a sequence of stimuli that steps incrementally from a good instance of one category to a good instance of another.","keywords":["P-Lab"],"articleBody":"Phoneme Categorization In a phoneme categorization experiment, a listener is presented with a single stimulus on each trial, and they have to assign that stimulus to one or another category. It’s possible that the listener may be given more than just two categories to choose from but that’s rare in the experiments run in our lab.\nUsually, each stimulus is one of a sequence of stimuli that steps incrementally from a good instance of one category to a good instance of another. For example, a stimulus might be drawn from a sequence stepping incrementally from /s/ and /sh/. Steps in this sequence that are distant from both the /s/ and /sh/ endpoints are ambiguous between the categories, and the listener has to guess which category to assign them to. The sequence is often referred to as a “continuum,” despite the fact that its members are discrete steps between the endpoints. It’s referred to as a continuum because listeners can’t reliably distinguish adjacent steps in the sequence from one another.\nIt’s possible to study listeners’ categorization of such a continuum without any other independent manipulations, in which case, the questions of interest are:\n where along the sequence do listeners cross over from assigning stimuli to one category to assigning it the other, how abruptly do they cross over, and how quickly they respond to each step along the continuum.  More often, and nearly always in our lab, we also manipulate the context in which the sequence occurs independently to test how differences between contexts influence any of where listeners cross over, how abruptly they do so, and how fast they categorize the stimulus. When we manipulate the context independently like this, the portion of the stimulus in which members of the continuum occur is called the “target.”\nWhen the categorization experiment is actually run, a stimulus is drawn at random from the target continuum or the context+target combinations on each trial and presented to the listener who responds by choosing one or the other category.\nStimuli are constructed in a number of ways. The most typical is described here:\n  Record naturally produced instances of the target categories, in the contexts that you want to manipulate,\n  Measure the acoustic properties of these sounds (durations, intensities, fundamental frequencies, formant frequencies and bandwidths),\n  Edit and smooth those values,\n  Calculate the values each property should have for the intermediate steps between the endpoint categories,\n  Supply them as parameter values to a synthesizer, which then produces the steps along the continuum.\nThere is one general exception to this procedure in our everyday practice: continua between noisy categories, like the /s-sh/ continuum mentioned above, are produced by adding the original sounds’ waveforms together in complementary proportions. For example, the /s/ endpoint would be 1.0 /s/ + 0.0 /sh/, the next step would be 0.9 /s/ + 0.1 /sh/, …, 0.1 /s/ + 0.9 /sh/, finally to 0.0 /s/ + 1.0 /sh/ at the /sh/ endpoint.\nAll these steps are currently carried out using a combination of Praat and R scripts. The most commonly used suite of Praat scripts is included below, along with an example R script. We continue to work toward a routine as opposed to artisanal pipeline for constructing stimuli.\n  Stimulus presentation and response collection is handled with PsychToolBox scripts running in octave. Responses are collected using purpose-built button boxes. An example script with the necessary libraries is also included below.\nOnce the data are collected:\n the files containing the responses from each listener are compiled, the data are cleaned up (purging trials with invalid responses, with no response, outlying response times, etc.), responses are coded to represent the conditions in the experiment, plotted using ggplot and modeled using lmer for RTs and glmer for response proportions.  All this is done in R/RStudio. An example RMarkdown script showing these steps is included below, along with the data files to which it was applied.\n","wordCount":"648","inLanguage":"en","image":"https://xlingumass.github.io/%3Cimage%20path/url%3E","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"Christina Muxica"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://xlingumass.github.io/docs/phonetics/"},"publisher":{"@type":"Organization","name":"XLing Labs | UMass","logo":{"@type":"ImageObject","url":"https://www.umass.edu/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><script defer src=/static/fontawesome/fontawesome-all.js></script><header class=header><nav class=nav><div class=logo><a href=https://xlingumass.github.io accesskey=h title="XLing Labs (Alt + H)">XLing Labs</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://xlingumass.github.io/schedule/ title="Meeting Schedule"><span>Meeting Schedule</span></a></li><li><a href=https://xlingumass.github.io/labs/ title="Joint Labs"><span>Joint Labs</span></a></li><li><a href=https://xlingumass.github.io/docs/ title=Docs><span>Docs</span></a></li><li><a href=https://xlingumass.github.io/blog/ title=Blog><span>Blog</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://xlingumass.github.io>Home</a>&nbsp;»&nbsp;<a href=https://xlingumass.github.io/docs/>Docs</a></div><h1 class=post-title>Phonetics Guides</h1><div class=post-meta>Christina Muxica&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/docs/phonetics.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#phoneme-categorization aria-label="Phoneme Categorization">Phoneme Categorization</a></li></ul></div></details></div><div class=post-content><h2 id=phoneme-categorization>Phoneme Categorization<a hidden class=anchor aria-hidden=true href=#phoneme-categorization>#</a></h2><p>In a phoneme categorization experiment, a listener is presented with a single stimulus on each trial, and they have to assign that stimulus to one or another category. It&rsquo;s possible that the listener may be given more than just two categories to choose from but that&rsquo;s rare in the experiments run in our lab.</p><p>Usually, each stimulus is one of a sequence of stimuli that steps incrementally from a good instance of one category to a good instance of another. For example, a stimulus might be drawn from a sequence stepping incrementally from /s/ and /sh/. Steps in this sequence that are distant from both the /s/ and /sh/ endpoints are <em>ambiguous</em> between the categories, and the listener has to guess which category to assign them to. The sequence is often referred to as a &ldquo;continuum,&rdquo; despite the fact that its members are discrete steps between the endpoints. It&rsquo;s referred to as a continuum because listeners can&rsquo;t reliably distinguish adjacent steps in the sequence from one another.</p><p>It&rsquo;s possible to study listeners&rsquo; categorization of such a continuum without any other independent manipulations, in which case, the questions of interest are:</p><ul><li>where along the sequence do listeners cross over from assigning stimuli to one category to assigning it the other,</li><li>how abruptly do they cross over, and</li><li>how quickly they respond to each step along the continuum.</li></ul><p>More often, and nearly always in our lab, we also manipulate the context in which the sequence occurs independently to test how differences between contexts influence any of where listeners cross over, how abruptly they do so, and how fast they categorize the stimulus. When we manipulate the context independently like this, the portion of the stimulus in which members of the continuum occur is called the &ldquo;target.&rdquo;</p><p>When the categorization experiment is actually run, a stimulus is drawn at random from the target continuum or the context+target combinations on each trial and presented to the listener who responds by choosing one or the other category.</p><p>Stimuli are constructed in a number of ways. The most typical is described here:</p><ul><li><p>Record naturally produced instances of the target categories, in the contexts that you want to manipulate,</p></li><li><p>Measure the acoustic properties of these sounds (durations, intensities, fundamental frequencies, formant frequencies and bandwidths),</p></li><li><p>Edit and smooth those values,</p></li><li><p>Calculate the values each property should have for the intermediate steps between the endpoint categories,</p></li><li><p>Supply them as parameter values to a synthesizer, which then produces the steps along the continuum.</p><p>There is one general exception to this procedure in our everyday practice: continua between noisy categories, like the /s-sh/ continuum mentioned above, are produced by adding the original sounds&rsquo; waveforms together in complementary proportions. For example, the /s/ endpoint would be 1.0 /s/ + 0.0 /sh/, the next step would be 0.9 /s/ + 0.1 /sh/, &mldr;, 0.1 /s/ + 0.9 /sh/, finally to 0.0 /s/ + 1.0 /sh/ at the /sh/ endpoint.</p><p>All these steps are currently carried out using a combination of Praat and R scripts. The most commonly used suite of Praat scripts is included below, along with an example R script. We continue to work toward a routine as opposed to artisanal pipeline for constructing stimuli.</p></li></ul><p>Stimulus presentation and response collection is handled with PsychToolBox scripts running in octave. Responses are collected using purpose-built button boxes. An example script with the necessary libraries is also included below.</p><p>Once the data are collected:</p><ul><li>the files containing the responses from each listener are compiled,</li><li>the data are cleaned up (purging trials with invalid responses, with no response, outlying response times, etc.),</li><li>responses are coded to represent the conditions in the experiment,</li><li>plotted using <code>ggplot</code> and</li><li>modeled using <code>lmer</code> for RTs and <code>glmer</code> for response proportions.</li></ul><p>All this is done in R/RStudio. An example RMarkdown script showing these steps is included below, along with the data files to which it was applied.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://xlingumass.github.io/tags/p-lab/>P-Lab</a></li></ul><nav class=paginav><a class=prev href=https://xlingumass.github.io/docs/osf/><span class=title>« Prev Page</span><br><span>OSF</span></a>
<a class=next href=https://xlingumass.github.io/docs/sona/><span class=title>Next Page »</span><br><span>Sona</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://xlingumass.github.io>XLing Labs | UMass</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(t=>{const n=t.parentNode.parentNode,e=document.createElement("button");e.classList.add("copy-code"),e.innerText="copy";function s(){e.innerText="copied!",setTimeout(()=>{e.innerText="copy"},2e3)}e.addEventListener("click",o=>{if("clipboard"in navigator){navigator.clipboard.writeText(t.textContent),s();return}const e=document.createRange();e.selectNodeContents(t);const n=window.getSelection();n.removeAllRanges(),n.addRange(e);try{document.execCommand("copy"),s()}catch(e){}n.removeRange(e)}),n.classList.contains("highlight")?n.appendChild(e):n.parentNode.firstChild==n||(t.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?t.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(e):t.parentNode.appendChild(e))})</script></body></html>